# 参数范数惩罚

## L2 参数正则化

$$
\tilde J=J+\lambda \Omega(\mathbf w)=J+\frac{\alpha}{2} \mathbf w^T\mathbf w\\
\nabla_{\mathbf w} \tilde J=\nabla_{\mathbf w}  J+ \alpha\mathbf w\\
\Rightarrow \mathbf w\leftarrow \mathbf w-\epsilon  \alpha \mathbf w-\epsilon \nabla_{\mathbf w} J
$$

假设  $\mathbf w^*=\arg \min J$ ,  在 $\mathbf w$  附近，loss function 可以被近似的泰勒级数二阶展开


$$
J(\mathbf w)=J(\mathbf w^*)+\overbrace{\nabla_{\mathbf w} J(\mathbf w^*)}^{=0}(\mathbf w-\mathbf w^*)+\frac{1}{2}(\mathbf w-\mathbf w^*)^T \mathbf H (\mathbf w-\mathbf w^*)\\
\Rightarrow \nabla_{\mathbf w} J(\mathbf w)= \mathbf H (\mathbf w-\mathbf w^*)
$$
如果有正则化，假设此时$\tilde {\mathbf w}$ 是最优解  那么
$$
\nabla_{\tilde {\mathbf w}}\tilde J=\alpha \tilde {\mathbf w}+\mathbf H (\tilde {\mathbf w}-\mathbf w^*)=0\\
\Rightarrow \tilde {\mathbf w}=(\alpha I+\mathbf H)^{-1}\mathbf H \mathbf w^*\\\
=(\alpha I+V\Lambda V^T)^{-1}V\Lambda V^T \mathbf w^*\\
=(V(\Lambda+\alpha I) V^T)^{-1}V\Lambda V^T \mathbf w^*\\
= V(\Lambda^{-1}+\alpha^{-1} I) V^T V\Lambda V^T \mathbf w^*\\
=V (\Lambda^{-1}+\alpha^{-1} I)\Lambda V^T\mathbf w^*
$$
当 α 趋向于 0 时，正则化的解  $\tilde {\mathbf w}$ 会趋向   ${\mathbf w^*}$ , 否则当 $\alpha$ 比较大，将会以$\frac{\lambda_i}{\lambda_i+\alpha}$   沿着特征向量缩放${\mathbf w^*}$ 

对于特征值很小的方向，将会被忽略，特征值大的，将不会受到影响。

### L2正则化影响

  ${\mathbf w^*}$  会养着 特征值小的特征方向拼命移动 趋向于0，因为在这个方向移动 J(w)增大很小，但是正则相会剧烈下降。

## L1 参数正则化

$$
\tilde J=J+\lambda \Omega(\mathbf w)=J+\alpha ||\mathbf w||_1\\
\nabla_{\mathbf w} \tilde J=\nabla_{\mathbf w}  J+ \alpha\mbox{sign}(\mathbf w)\\
\Rightarrow \mathbf w\leftarrow \mathbf w-\epsilon \alpha\mbox{sign}(\mathbf w)-\epsilon \nabla_{\mathbf w} J\\
$$

假设 $\mathbf w^*=\arg \min J$ ,  在 $\mathbf w$  附近，loss function 可以被近似的泰勒级数二阶展开
$$
J(\mathbf w)=J(\mathbf w^*)+\overbrace{\nabla_{\mathbf w} J(\mathbf w^*)}^{=0}(\mathbf w-\mathbf w^*)+\frac{1}{2}(\mathbf w-\mathbf w^*)^T \mathbf H (\mathbf w-\mathbf w^*)\\
\Rightarrow \nabla_{\mathbf w} J(\mathbf w)= \mathbf H (\mathbf w-\mathbf w^*)\\
$$
假设 H 是对角矩阵的来简单化表示,然后minimize piecewisely
$$
\Rightarrow  \hat J(\tilde {\mathbf w})= J(\mathbf w^*)+\sum_i \left(0.5\mathbf H_{i,i}(\tilde {\mathbf w_i}-\mathbf w_i^*)^2+\alpha|\tilde {\mathbf w}_i|\right)\\
\mbox{minimize piecewisely    } \alpha|\tilde {\mathbf w}_i|+ 0.5\mathbf H_{i,i}(\tilde {\mathbf w}_i^2+ {\mathbf w}_i^{*2}-2\tilde {\mathbf w}_i {\mathbf w}_i^*)\\
\partial L/\partial \tilde {\mathbf w}_i = \mathbf H_{i,i}( \tilde {\mathbf w}_i - {\mathbf w}_i^*)+\alpha\mbox{sign}(\tilde {\mathbf w}_i)=\mathbf H_{i,i}\tilde {\mathbf w}_i - \mathbf H_{i,i}{\mathbf w}_i^*+\alpha\mbox{sign}(\tilde {\mathbf w}_i)\\

\Rightarrow \tilde {\mathbf w}_i={\mathbf w}_i^*- \frac{\alpha\mbox{sign}(\tilde {\mathbf w}_i)}{\mathbf H_{i,i}} \mbox{or} 0
$$

###L1正则化影响

 对于比较小的特征值对应的向量，会取0，而不像l2的压缩





#Bagging 

### 模型平均(model averaging)

奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。

假设每个模型在每个例子上的误差是 $\epsilon_i$  服从零均值,方差为 $\mathbb E[\epsilon_i^2]=v$ , 且协􏰘方差为 $\mathbb E[\epsilon_i\epsilon_j]=c$ ,的多维正态分布, 通过所有集成模型的平均预测所得误差是 $\frac{\sum_{i=1}^k \epsilon_i}{k}$ ,  集成模型的预测方差是
$$
\mathbb E\left[\left(\frac{\sum_{i=1}^k \epsilon_i-0}{k}\right)^2\right]=\frac{1}{k^2}\mathbb E\left[\left(\sum_{i=1}^k \epsilon_i\right)^2\right]=\frac{1}{k^2}\left(\sum_{i} v+\sum_{i,j:i\neq j}c\right)=\frac{v}{k}+\frac{(k-1)c}k{}
$$
而随机采样的顺利样本生成的模型，对单个样本的协方差 应该不会完全相关。

#### 推断(inference)

$$
p(y|\mathbf x)=\frac{1}{k}\sum_i p_i(y|\mathbf x)
$$

#Dropout

Dropout可以被认为是集成大量深层神 经网络提供廉价的Bagging集成近似。

### 乘零的简单Dropout算法

1. 我们每次在小批量中加载一 个样本
2. 以p为概率，随机抽样应用于网络中所有输入和隐藏单元是否乘以0来无效它   $\mathbf \mu=[0,1,0,1..]$ , 此时训练出的模型的代价就是$J(\theta,\mathbf \mu)$

在Dropout的情况下，所有模型共享参数，其中每个模型继承父神经网络参数 的不同子集。参数共享使得在有限可用的内存下表示指数级数量的模型变得可能。

#### VS bagging

- 在Bagging的情况下，每一个模型在其相应训练集上训练到收敛。 
- 在Dropout的情况下，通常大部分模型都没有显式地被训练，因为通常父神经网络会很大。取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。



####推断

通过掩码 μ 定义每个子模型的概率分布 $p(y|\mathbf x,\mu)$ , 
$$
p(y|\mathbf x)=\sum_{\mu} p(\mu)p(y|\mathbf x,\mu)
$$
$p(\mu)$ 是训练时候$\mu$采样概率。这个求和包含多达指数级的项，所以通常就是选几个掩 码就足以获得不错的表现 



#### 近似整个集成的推断

要做到这一点，我们改用集成成员预测分布的几何平均而不是算术平均。 Warde-Farley et al. (2014) 提出的论点和经验证据表明，在这个情况下几何平均与 算术平均表现得差不多。

几何平均直接定义的非标准化概率分布：
$$
\tilde p_{ensemble}(y|\mathbf x)=\left(\prod_{\mu}p(y|\mathbf x,\mu)\right)^{1/2^d}\\
\mbox{最终的集成推断：}p_{ensemble}(y|\mathbf x)=\frac{\tilde p_{ensemble}(y|\mathbf x)}{\sum_{y'}\tilde p_{ensemble}(y'|\mathbf x)}
$$


涉及Dropout的一个重要观点 (Hinton et al., 2012b) 是，我们可以通过评估模型 中 $p(y | \mathbf x)$ 来近似 $p_{ensemble}(y|\mathbf x)$ , i.e., $p(y|\mathbf x)\approx p_{ensemble}(y|\mathbf x)$

> 以softmax为例子证明
> $$
> p(\mathbf y=y|\mathbf x)=\mbox{softmax}(\mathbf w^T\mathbf x+b)_y=\frac{\exp(\mathbf w^T_y\mathbf x+b)}{\sum_{y'}\exp(\mathbf w^T_{y'}\mathbf x+b)}\\
> p(\mathbf y=y|\mathbf x,\mu)=\mbox{softmax}(\mathbf w^T(\mu\odot\mathbf x)+b)_y\\
> \tilde p_{ensemble}(\mathbf y=y|\mathbf x)=\left(\prod_{\mu\in\{0,1\}^d}p(y|\mathbf x,\mu)\right)^{1/2^d}=\left(\prod_{\mu\in\{0,1\}^d}\mbox{softmax}(\mathbf w^T(\mu\odot\mathbf x)+b)_y\right)^{1/2^d}\\
> =\left(\prod_{\mu\in\{0,1\}^d}\frac{\exp(\mathbf w^T_y(\mu\odot\mathbf x)+b)}{\sum_{y'}\exp(\mathbf w^T_{y'}(\mu\odot\mathbf x)+b)}\right)^{1/2^d}\\
> \mbox{因为最终会被归一化，所以分母不用考虑}\Rightarrow \\
> \tilde p_{ensemble}(\mathbf y=y|\mathbf x)\propto \left(\prod_{\mu\in\{0,1\}^d}\exp(\mathbf w^T_y(\mu\odot\mathbf x)+b)\right)^{1/2^d}\\
> =\exp \left( \frac{1}{2^d}\sum_{\mu} (\mathbf w^T_y(\mu\odot\mathbf x)+b)\right)\overbrace{=}^{\mbox{假设以0.5概率采样u}}\exp \left( \frac{1}{2} \mathbf w^T_y\mathbf x+b\right)
> $$
>

#####最终得到的是权重是0.5的softmax分类器，所以只要把所有参数处以2， 就可以用整个模型的输出，来近似代替集成模型的输出了。 当然这个集成模型就几何平均模型，不是算术平均，然而实验中表现结果差不多。

Dropout强大的大部分原因来自施加到隐藏单元的掩码噪声，了解这一事实是重要的。这可以看作是对输入内容的信息高度智能化、自适应破坏的一种形式，而不 是对输入原始值的破坏。例如，如果模型学得通过鼻检测脸的隐藏单元 hi，那么丢 失 hi 对应于擦除图像中有鼻子的信息。模型必须学习另一种 hi，要么是鼻子存在的 冗余编码，要么是脸部的另一特征，如嘴。



传统的噪声注入技术，在输入端加非结 构化的噪声不能够随机地从脸部图像中抹去关于鼻子的信息，除非噪声的幅度大到 几乎能抹去图像中所有的信息。破坏提取的特征而不是原始值，让破坏过程充分利 用该模型迄今获得的关于输入分布的所有知识。



